{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a3799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from importlib import reload\n",
    "# import feature_extraction\n",
    "# reload(feature_extraction)\n",
    "from plotting import (\n",
    "    plot_series,\n",
    "    plot_eeg_topomap_one_block,\n",
    "    plot_eeg_topomap_all_blocks,\n",
    "    plot_k_chaneels_by_r_value,\n",
    "    plot_eeg_pearson_correlation_table,\n",
    "    get_eeg_pearson_correlation_series_all_blocks,\n",
    "    plot_time_series_by_epoch,\n",
    "    plot_scatter_by_marker,\n",
    ")\n",
    "from biomarkers import (\n",
    "    EEG,\n",
    "    EMG,\n",
    "    BP,\n",
    "    EOG,\n",
    "    ECG,\n",
    "    TREV,\n",
    "    GSR,\n",
    ")\n",
    "from feature_extraction import (\n",
    "    Feature,\n",
    "    EEG_BANDS,\n",
    "    FEATURE_TO_FUNC,\n",
    ")\n",
    "from data_utils import (\n",
    "    load_data_from_dir,\n",
    "    get_all_behaviors_labels,\n",
    "    get_all_features_by_marker,\n",
    "    extract_labels,\n",
    "    extract_features_by_channel,\n",
    ")\n",
    "from calculate_correlation import (\n",
    "    STAT_FEATURES,\n",
    "    get_all_behaviors_feature_to_pc_by_markers,\n",
    "    get_all_trials_average_rp_values,\n",
    ")\n",
    "\n",
    "# Train machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import (\n",
    "    GroupKFold,\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f4b9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete loading 10 markers\n",
      "Loaded audio_hvla block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_nvha block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_hvha block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_nvla block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_hvla block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_nvha block\n",
      "Complete loading 10 markers\n",
      "Loaded mind wandering block\n",
      "Complete loading 10 markers\n",
      "Loaded meditation block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_hvha block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_nvla block\n",
      "Complete loading 10 markers\n",
      "Loaded mind wandering block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_hvla block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_hvla block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_nvla block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_nvla block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_nvha block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_nvha block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_hvha block\n",
      "Complete loading 10 markers\n",
      "Loaded meditation block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_hvha block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_hvla block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_hvha block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_nvha block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_nvla block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_nvla block\n",
      "Complete loading 10 markers\n",
      "Loaded mind wandering block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_nvha block\n",
      "Complete loading 10 markers\n",
      "Loaded audio_hvha block\n",
      "Complete loading 10 markers\n",
      "Loaded meditation block\n",
      "Complete loading 10 markers\n",
      "Loaded breath_hvla block\n"
     ]
    }
   ],
   "source": [
    "ALL_DIRS = [\"../2000_CleanData\", \"../2001_CleanData\", \"../1004_CleanData\"]\n",
    "\n",
    "dir_to_data = {}\n",
    "for dir_name in ALL_DIRS:\n",
    "    all_data = load_data_from_dir(dir_name)\n",
    "    dir_to_data[dir_name] = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7a1aa205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def binary_label(all_labels, threshold=0.5):\n",
    "    y_transformed = []\n",
    "    c0, c1 = 0, 0\n",
    "    for label in all_labels:\n",
    "        if label < threshold:\n",
    "            y_transformed.append(0)\n",
    "            c0 += 1\n",
    "        else:\n",
    "            y_transformed.append(1)\n",
    "            c1 += 1\n",
    "\n",
    "    print(f\"0 label: {c0}, 1 label: {c1}\")\n",
    "    return y_transformed\n",
    "\n",
    "\n",
    "def get_tranformed_labels(condition_to_labels: dict):\n",
    "    transformed = dict()\n",
    "    for condition, label in condition_to_labels.items():\n",
    "        transformed[condition] = binary_label(label)\n",
    "    return transformed\n",
    "\n",
    "def get_categorical_labels(condition_to_labels: dict, threshold=0.5):\n",
    "    valence_labels = condition_to_labels['valence']\n",
    "    arousal_labels = condition_to_labels['arousal']\n",
    "    claz = []\n",
    "    for i, v_label in enumerate(valence_labels):\n",
    "       if arousal_labels[i] <= threshold and v_label <= threshold: # nvla\n",
    "         claz.append(0)\n",
    "       elif arousal_labels[i] > threshold and v_label <= threshold: # nvha\n",
    "         claz.append(1)\n",
    "       elif arousal_labels[i] <= threshold and v_label > threshold: # hvla\n",
    "         claz.append(2)\n",
    "       else: # hvha\n",
    "         claz.append(3)            \n",
    "        \n",
    "    return claz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6aa0ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 12288, 130) 3\n",
      "All labels shape: (130,)\n",
      "All labels shape: (130,)\n",
      "All labels shape: (130,)\n",
      "(128, 12288, 130) 3\n",
      "All labels shape: (130,)\n",
      "All labels shape: (130,)\n",
      "All labels shape: (130,)\n",
      "(128, 12288, 130) 3\n",
      "All labels shape: (130,)\n",
      "All labels shape: (130,)\n",
      "All labels shape: (130,)\n"
     ]
    }
   ],
   "source": [
    "from data_utils import concatenate_all_data, get_sorted_block_to_data_by_marker, get_sorted_behavior_labels\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from biomarkers import (\n",
    "    EGG\n",
    ")\n",
    "from typing import Tuple\n",
    "\n",
    "def _concatenate_all_data(dir_to_data: dict, marker: str) -> Tuple[np.ndarray, dict]:\n",
    "    all_participants_data = np.array([])\n",
    "    condition_to_labels = {\"valence\": [], \"arousal\": [], \"attention\": []}\n",
    "    for _, data in dir_to_data.items():\n",
    "        block_names = list(data.keys())\n",
    "        block_names.sort()\n",
    "\n",
    "        sorted_data = get_sorted_block_to_data_by_marker(data, marker, block_names)\n",
    "        sorted_data = np.expand_dims(sorted_data, axis=0) if sorted_data.ndim == 2 else sorted_data\n",
    "        print(sorted_data.shape, sorted_data.ndim)\n",
    "        all_participants_data = (\n",
    "            sorted_data\n",
    "            if all_participants_data.ndim == 1\n",
    "            else np.concatenate((all_participants_data, sorted_data), axis=-1)\n",
    "        )\n",
    "\n",
    "        for condition, _ in condition_to_labels.items():\n",
    "            labels = get_sorted_behavior_labels(data, condition, block_names)\n",
    "            condition_to_labels[condition].extend(labels)\n",
    "\n",
    "    return all_participants_data, condition_to_labels\n",
    "\n",
    "data_list = []\n",
    "for m in [EEG.__name__]: # EGG.__name__,EOG.__name__, EMG.__name__,\n",
    "    all_participants_data, condition_to_labels = _concatenate_all_data(dir_to_data, m)\n",
    "    all_epoch_data = np.swapaxes(\n",
    "    all_participants_data, 0, -1\n",
    "    )  # (num_channels, num_data_points, num_epochs) => (num_epochs, num_data_points, num_channels)\n",
    "\n",
    "    data_list.append(all_epoch_data)\n",
    "\n",
    "data_array = np.concatenate(data_list, axis=2)\n",
    " \n",
    "# (TODO) combine attention + arousal to 2*2*2 = 8 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "412befc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 12286, 5)          1925      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 12286, 5)         20        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 12286, 5)          0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 6143, 5)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 6141, 5)           80        \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 6141, 5)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 3070, 5)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3070, 5)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 3068, 5)           80        \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 3068, 5)           0         \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 1534, 5)          0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1534, 5)           0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1532, 5)           80        \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 1532, 5)           0         \n",
      "                                                                 \n",
      " average_pooling1d_1 (Averag  (None, 766, 5)           0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 764, 5)            80        \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 764, 5)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 5)                0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 24        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,289\n",
      "Trainable params: 2,279\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,LeakyReLU,MaxPool1D,\\\n",
    "GlobalAveragePooling1D,Dense,Dropout,AveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "def cnn_model():\n",
    "    clear_session()\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=5,kernel_size=3,strides=1,input_shape=(12288, 128)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        MaxPool1D(pool_size=2,strides=2),\n",
    "        Conv1D(filters=5,kernel_size=3,strides=1),\n",
    "        LeakyReLU(),\n",
    "        MaxPool1D(pool_size=2,strides=2),\n",
    "        Dropout(0.5),\n",
    "        Conv1D(filters=5,kernel_size=3,strides=1),\n",
    "        LeakyReLU(),\n",
    "        AveragePooling1D(pool_size=2,strides=2),\n",
    "        Dropout(0.5),\n",
    "        Conv1D(filters=5,kernel_size=3,strides=1),\n",
    "        LeakyReLU(),\n",
    "        AveragePooling1D(pool_size=2,strides=2),\n",
    "        Conv1D(filters=5,kernel_size=3,strides=1),\n",
    "        LeakyReLU(),     \n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(4,activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile('adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "782e4879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 0, 2, 2, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 1, 2, 3, 1, 3, 2, 3, 2, 3, 2, 3, 2, 0, 0, 2, 2, 2, 3, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 1, 2, 1, 0, 0, 1, 1, 1, 2, 0, 3, 1, 1, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 3, 3, 3, 3, 3, 3, 3, 1, 0, 1, 2, 3, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 3, 2, 2, 2, 2, 2]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "(390, 12288, 128) (390, 4) (390,)\n"
     ]
    }
   ],
   "source": [
    "# to one-hot encoding vector\n",
    "label_list = get_categorical_labels(condition_to_labels)\n",
    "groups_list = [[i] for i, j in enumerate(label_list)]\n",
    "group_array = np.hstack(groups_list)\n",
    "print(label_list)\n",
    "label_array = np_utils.to_categorical(label_list, num_classes=4)\n",
    "#label_array = np.array(label_list['arousal'])\n",
    "\n",
    "print(label_array)\n",
    "print(data_array.shape, label_array.shape, group_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4f925ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "7/7 [==============================] - 5s 382ms/step - loss: 1.5188 - accuracy: 0.0705 - val_loss: 1.4757 - val_accuracy: 0.0513\n",
      "Epoch 2/13\n",
      "7/7 [==============================] - 2s 282ms/step - loss: 1.4266 - accuracy: 0.0705 - val_loss: 1.4175 - val_accuracy: 0.0513\n",
      "Epoch 3/13\n",
      "7/7 [==============================] - 2s 281ms/step - loss: 1.3904 - accuracy: 0.1506 - val_loss: 1.3790 - val_accuracy: 0.3077\n",
      "Epoch 4/13\n",
      "7/7 [==============================] - 2s 303ms/step - loss: 1.3684 - accuracy: 0.3942 - val_loss: 1.3490 - val_accuracy: 0.5385\n",
      "Epoch 5/13\n",
      "7/7 [==============================] - 2s 286ms/step - loss: 1.3498 - accuracy: 0.4776 - val_loss: 1.3244 - val_accuracy: 0.5385\n",
      "Epoch 6/13\n",
      "7/7 [==============================] - 2s 289ms/step - loss: 1.3326 - accuracy: 0.4968 - val_loss: 1.3027 - val_accuracy: 0.4359\n",
      "Epoch 7/13\n",
      "7/7 [==============================] - 2s 288ms/step - loss: 1.3151 - accuracy: 0.4647 - val_loss: 1.2799 - val_accuracy: 0.3590\n",
      "Epoch 8/13\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 1.2964 - accuracy: 0.4199 - val_loss: 1.2558 - val_accuracy: 0.3590\n",
      "Epoch 9/13\n",
      "7/7 [==============================] - 2s 275ms/step - loss: 1.2755 - accuracy: 0.4135 - val_loss: 1.2321 - val_accuracy: 0.3590\n",
      "Epoch 10/13\n",
      "7/7 [==============================] - 2s 285ms/step - loss: 1.2530 - accuracy: 0.4135 - val_loss: 1.2029 - val_accuracy: 0.3590\n",
      "Epoch 11/13\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 1.2266 - accuracy: 0.4103 - val_loss: 1.1706 - val_accuracy: 0.3590\n",
      "Epoch 12/13\n",
      "7/7 [==============================] - 2s 283ms/step - loss: 1.2002 - accuracy: 0.4135 - val_loss: 1.1421 - val_accuracy: 0.3590\n",
      "Epoch 13/13\n",
      "7/7 [==============================] - 2s 270ms/step - loss: 1.1733 - accuracy: 0.4071 - val_loss: 1.1123 - val_accuracy: 0.3590\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 1.1123 - accuracy: 0.3590\n",
      "Epoch 1/13\n",
      "7/7 [==============================] - 4s 339ms/step - loss: 1.4243 - accuracy: 0.1955 - val_loss: 1.2893 - val_accuracy: 0.4487\n",
      "Epoch 2/13\n",
      "7/7 [==============================] - 2s 287ms/step - loss: 1.3457 - accuracy: 0.3910 - val_loss: 1.2276 - val_accuracy: 0.4487\n",
      "Epoch 3/13\n",
      "7/7 [==============================] - 2s 275ms/step - loss: 1.2883 - accuracy: 0.3910 - val_loss: 1.1822 - val_accuracy: 0.4487\n",
      "Epoch 4/13\n",
      "7/7 [==============================] - 2s 286ms/step - loss: 1.2346 - accuracy: 0.3910 - val_loss: 1.1550 - val_accuracy: 0.4487\n",
      "Epoch 5/13\n",
      "7/7 [==============================] - 2s 277ms/step - loss: 1.1834 - accuracy: 0.3910 - val_loss: 1.1443 - val_accuracy: 0.4487\n",
      "Epoch 6/13\n",
      "7/7 [==============================] - 2s 282ms/step - loss: 1.1406 - accuracy: 0.3910 - val_loss: 1.1411 - val_accuracy: 0.4487\n",
      "Epoch 7/13\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 1.1044 - accuracy: 0.3910 - val_loss: 1.1383 - val_accuracy: 0.4487\n",
      "Epoch 8/13\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 1.0873 - accuracy: 0.3910 - val_loss: 1.1361 - val_accuracy: 0.4487\n",
      "Epoch 9/13\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 1.0705 - accuracy: 0.3910 - val_loss: 1.1299 - val_accuracy: 0.4487\n",
      "Epoch 10/13\n",
      "7/7 [==============================] - 2s 284ms/step - loss: 1.0606 - accuracy: 0.3910 - val_loss: 1.1274 - val_accuracy: 0.3974\n",
      "Epoch 11/13\n",
      "7/7 [==============================] - 2s 272ms/step - loss: 1.0577 - accuracy: 0.4615 - val_loss: 1.1227 - val_accuracy: 0.3974\n",
      "Epoch 12/13\n",
      "7/7 [==============================] - 2s 272ms/step - loss: 1.0522 - accuracy: 0.4904 - val_loss: 1.1167 - val_accuracy: 0.3974\n",
      "Epoch 13/13\n",
      "7/7 [==============================] - 2s 266ms/step - loss: 1.0473 - accuracy: 0.4904 - val_loss: 1.1104 - val_accuracy: 0.3974\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.1104 - accuracy: 0.3974\n",
      "Epoch 1/13\n",
      "7/7 [==============================] - 4s 402ms/step - loss: 1.3438 - accuracy: 0.0673 - val_loss: 1.2997 - val_accuracy: 0.4872\n",
      "Epoch 2/13\n",
      "7/7 [==============================] - 2s 300ms/step - loss: 1.2960 - accuracy: 0.4647 - val_loss: 1.2455 - val_accuracy: 0.4872\n",
      "Epoch 3/13\n",
      "7/7 [==============================] - 2s 276ms/step - loss: 1.2508 - accuracy: 0.4679 - val_loss: 1.1950 - val_accuracy: 0.4872\n",
      "Epoch 4/13\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 1.2148 - accuracy: 0.4679 - val_loss: 1.1526 - val_accuracy: 0.4872\n",
      "Epoch 5/13\n",
      "7/7 [==============================] - 2s 272ms/step - loss: 1.1789 - accuracy: 0.4679 - val_loss: 1.1316 - val_accuracy: 0.4872\n",
      "Epoch 6/13\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 1.1591 - accuracy: 0.4679 - val_loss: 1.1165 - val_accuracy: 0.4872\n",
      "Epoch 7/13\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 1.1374 - accuracy: 0.4679 - val_loss: 1.1034 - val_accuracy: 0.4872\n",
      "Epoch 8/13\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 1.1189 - accuracy: 0.4679 - val_loss: 1.0990 - val_accuracy: 0.4872\n",
      "Epoch 9/13\n",
      "7/7 [==============================] - 2s 287ms/step - loss: 1.1048 - accuracy: 0.4679 - val_loss: 1.0930 - val_accuracy: 0.4872\n",
      "Epoch 10/13\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 1.0886 - accuracy: 0.4679 - val_loss: 1.0798 - val_accuracy: 0.4872\n",
      "Epoch 11/13\n",
      "7/7 [==============================] - 5s 775ms/step - loss: 1.0784 - accuracy: 0.4679 - val_loss: 1.0735 - val_accuracy: 0.4872\n",
      "Epoch 12/13\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.0676 - accuracy: 0.4679 - val_loss: 1.0695 - val_accuracy: 0.4872\n",
      "Epoch 13/13\n",
      "7/7 [==============================] - 2s 276ms/step - loss: 1.0586 - accuracy: 0.4679 - val_loss: 1.0643 - val_accuracy: 0.4872\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 1.0643 - accuracy: 0.4872\n",
      "Epoch 1/13\n",
      "7/7 [==============================] - 4s 359ms/step - loss: 1.4373 - accuracy: 0.0609 - val_loss: 1.4021 - val_accuracy: 0.0513\n",
      "Epoch 2/13\n",
      "7/7 [==============================] - 3s 487ms/step - loss: 1.3938 - accuracy: 0.1154 - val_loss: 1.3829 - val_accuracy: 0.2564\n",
      "Epoch 3/13\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 1.3772 - accuracy: 0.4103 - val_loss: 1.3695 - val_accuracy: 0.4231\n",
      "Epoch 4/13\n",
      "7/7 [==============================] - 2s 362ms/step - loss: 1.3651 - accuracy: 0.4776 - val_loss: 1.3590 - val_accuracy: 0.4231\n",
      "Epoch 5/13\n",
      "7/7 [==============================] - 2s 333ms/step - loss: 1.3563 - accuracy: 0.4776 - val_loss: 1.3492 - val_accuracy: 0.4487\n",
      "Epoch 6/13\n",
      "7/7 [==============================] - 2s 302ms/step - loss: 1.3474 - accuracy: 0.4776 - val_loss: 1.3396 - val_accuracy: 0.4487\n",
      "Epoch 7/13\n",
      "7/7 [==============================] - 2s 283ms/step - loss: 1.3384 - accuracy: 0.4776 - val_loss: 1.3291 - val_accuracy: 0.4487\n",
      "Epoch 8/13\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 1.3280 - accuracy: 0.4776 - val_loss: 1.3163 - val_accuracy: 0.4487\n",
      "Epoch 9/13\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 1.3157 - accuracy: 0.4776 - val_loss: 1.2994 - val_accuracy: 0.4487\n",
      "Epoch 10/13\n",
      "7/7 [==============================] - 2s 277ms/step - loss: 1.2997 - accuracy: 0.4776 - val_loss: 1.2789 - val_accuracy: 0.4487\n",
      "Epoch 11/13\n",
      "7/7 [==============================] - 2s 281ms/step - loss: 1.2797 - accuracy: 0.4776 - val_loss: 1.2546 - val_accuracy: 0.4487\n",
      "Epoch 12/13\n",
      "7/7 [==============================] - 2s 273ms/step - loss: 1.2529 - accuracy: 0.4776 - val_loss: 1.2268 - val_accuracy: 0.4487\n",
      "Epoch 13/13\n",
      "7/7 [==============================] - 2s 274ms/step - loss: 1.2231 - accuracy: 0.4776 - val_loss: 1.2020 - val_accuracy: 0.4487\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.2020 - accuracy: 0.4487\n",
      "Epoch 1/13\n",
      "7/7 [==============================] - 6s 361ms/step - loss: 1.3760 - accuracy: 0.4135 - val_loss: 1.3641 - val_accuracy: 0.3590\n",
      "Epoch 2/13\n",
      "7/7 [==============================] - 2s 282ms/step - loss: 1.3525 - accuracy: 0.4135 - val_loss: 1.3411 - val_accuracy: 0.3590\n",
      "Epoch 3/13\n",
      "7/7 [==============================] - 2s 284ms/step - loss: 1.3275 - accuracy: 0.4135 - val_loss: 1.3177 - val_accuracy: 0.3590\n",
      "Epoch 4/13\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 1.3000 - accuracy: 0.4135 - val_loss: 1.2908 - val_accuracy: 0.3590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/13\n",
      "7/7 [==============================] - 2s 284ms/step - loss: 1.2661 - accuracy: 0.4135 - val_loss: 1.2611 - val_accuracy: 0.3590\n",
      "Epoch 6/13\n",
      "7/7 [==============================] - 2s 285ms/step - loss: 1.2292 - accuracy: 0.4135 - val_loss: 1.2315 - val_accuracy: 0.3590\n",
      "Epoch 7/13\n",
      "7/7 [==============================] - 2s 282ms/step - loss: 1.1868 - accuracy: 0.4135 - val_loss: 1.1984 - val_accuracy: 0.3590\n",
      "Epoch 8/13\n",
      "7/7 [==============================] - 2s 295ms/step - loss: 1.1511 - accuracy: 0.4135 - val_loss: 1.1665 - val_accuracy: 0.3590\n",
      "Epoch 9/13\n",
      "7/7 [==============================] - 2s 303ms/step - loss: 1.1175 - accuracy: 0.4135 - val_loss: 1.1438 - val_accuracy: 0.3590\n",
      "Epoch 10/13\n",
      "7/7 [==============================] - 2s 285ms/step - loss: 1.0868 - accuracy: 0.4135 - val_loss: 1.1241 - val_accuracy: 0.3590\n",
      "Epoch 11/13\n",
      "7/7 [==============================] - 2s 284ms/step - loss: 1.0704 - accuracy: 0.4135 - val_loss: 1.1080 - val_accuracy: 0.3590\n",
      "Epoch 12/13\n",
      "7/7 [==============================] - 2s 273ms/step - loss: 1.0543 - accuracy: 0.4135 - val_loss: 1.0996 - val_accuracy: 0.3590\n",
      "Epoch 13/13\n",
      "7/7 [==============================] - 2s 284ms/step - loss: 1.0460 - accuracy: 0.4872 - val_loss: 1.0942 - val_accuracy: 0.4615\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 1.0942 - accuracy: 0.4615\n"
     ]
    }
   ],
   "source": [
    "accuracy=[]\n",
    "gkf=GroupKFold()\n",
    "for train_index, val_index in gkf.split(data_array, label_array, groups=group_array):\n",
    "    train_features, train_labels=data_array[train_index], label_array[train_index]\n",
    "    val_features, val_labels=data_array[val_index], label_array[val_index]\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    model=cnn_model()\n",
    "    model.fit(train_features, train_labels, epochs=13, batch_size=50,validation_data=(val_features,val_labels))\n",
    "    accuracy.append(model.evaluate(val_features, val_labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "456249c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    File \"/Users/tpan/miniconda3/envs/MM/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py\", line 252, in grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n\n    LookupError: gradient registry has no entry for: shap_LeakyRelu\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [180]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m background \u001b[38;5;241m=\u001b[39m data_array[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(data_array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m100\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m      6\u001b[0m e \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model, background)\n\u001b[0;32m----> 7\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_array\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MM/lib/python3.8/site-packages/shap/explainers/_deep/__init__.py:124\u001b[0m, in \u001b[0;36mDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124;03m\"\"\" Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m        were chosen as \"top\".\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MM/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py:312\u001b[0m, in \u001b[0;36mTFDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[1;32m    311\u001b[0m feature_ind \u001b[38;5;241m=\u001b[39m model_output_ranks[j,i]\n\u001b[0;32m--> 312\u001b[0m sample_phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi_symbolic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_ind\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):\n",
      "File \u001b[0;32m~/miniconda3/envs/MM/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py:372\u001b[0m, in \u001b[0;36mTFDeep.run\u001b[0;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[1;32m    369\u001b[0m         tf_execute\u001b[38;5;241m.\u001b[39mrecord_gradient \u001b[38;5;241m=\u001b[39m tf_backprop\u001b[38;5;241m.\u001b[39mrecord_gradient\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_out\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_with_overridden_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43manon\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MM/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py:408\u001b[0m, in \u001b[0;36mTFDeep.execute_with_overridden_gradients\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# reinstate the backpropagatable check\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tf_gradients_impl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_IsBackpropagatable\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/MM/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py:365\u001b[0m, in \u001b[0;36mTFDeep.run.<locals>.anon\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m     v \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs[i]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    364\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[0;32m--> 365\u001b[0m final_out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     tf_execute\u001b[38;5;241m.\u001b[39mrecord_gradient \u001b[38;5;241m=\u001b[39m tf_backprop\u001b[38;5;241m.\u001b[39m_record_gradient\n",
      "File \u001b[0;32m~/miniconda3/envs/MM/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/MM/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1233\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1234\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    File \"/Users/tpan/miniconda3/envs/MM/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py\", line 252, in grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n\n    LookupError: gradient registry has no entry for: shap_LeakyRelu\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "accuracy\n",
    "\n",
    "# background = data_array[np.random.choice(data_array.shape[0], 100, replace=False)]\n",
    "# e = shap.DeepExplainer(model, background)\n",
    "# shap_values = e.shap_values(data_array[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2152ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = STAT_FEATURES\n",
    "marker = ECG.__name__\n",
    "\n",
    "\"\"\" extract features from physiological signals\n",
    "\"\"\"\n",
    "dir_name_to_features = extract_features_by_channel(marker, dir_to_data, features, 0)\n",
    "dir_name_to_labels = extract_labels(dir_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ebf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" extract correlation from physiological signals\n",
    "\"\"\"\n",
    "num_channels = 4\n",
    "num_blocks = 0\n",
    "dir_name_to_ctf = {}\n",
    "for dir_name, all_data in dir_to_data.items():\n",
    "    dir_name_to_ctf[dir_name] = get_all_behaviors_feature_to_pc_by_markers(\n",
    "        all_data, marker, features, num_channels, num_blocks\n",
    "    )\n",
    "    \n",
    "avg_condition_to_features = get_all_trials_average_rp_values(\n",
    "    dir_name_to_ctf, features, \"pearson\"\n",
    ")\n",
    "spearman_corr = get_all_trials_average_rp_values(dir_name_to_ctf, features, \"spearman\")\n",
    "for b, feature_to_pc in avg_condition_to_features.items():\n",
    "    for f, pc in feature_to_pc.items():\n",
    "        avg_condition_to_features[b][f] = np.hstack((pc, spearman_corr[b][f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = dir_to_data[\"../2000_CleanData\"]\n",
    "channel_names = all_data[\"audio_hvla\"].get_chanlocs(marker)\n",
    "\n",
    "\"\"\" plot the time series given the marker and block\n",
    "\"\"\"\n",
    "# plot_time_series_by_epoch(all_data['audio_hvla'], 'EMG', 'audio_hvla', 0)\n",
    "\n",
    "\n",
    "\"\"\" plot the single channel correlation table\n",
    "\"\"\"\n",
    "# channel = 3\n",
    "# for condition, feature_to_pc in avg_condition_to_features.items():\n",
    "#     label = f\"{channel_names[channel]} {condition}\"\n",
    "#     plot_correlation_table_by_channel(\n",
    "#         label, feature_to_pc, [\"pearson r\", \"pearson p\", \"spearman r\", \"spearman p\"], features, channel, True\n",
    "#     )\n",
    "\n",
    "\"\"\" plot the top channel correlation table given r values\n",
    "\"\"\"\n",
    "# for condition, feature_to_pc in avg_condition_to_features.items():\n",
    "#     plot_k_chaneels_by_r_value(feature_to_pc, channel_names, features, condition, True, 2)\n",
    "#     plot_k_chaneels_by_r_value(feature_to_pc, channel_names, features, condition, False, 2)\n",
    "\n",
    "\n",
    "\"\"\" plot the top channel correlation table with different blocks\n",
    "\"\"\"\n",
    "# all_block_names = list(all_data.keys())\n",
    "# all_block_names.sort()\n",
    "# for condition, feature_to_pc in avg_condition_to_features.items():\n",
    "#     plot_eeg_pearson_correlation_table(condition, feature_to_pc, all_block_names, 1)\n",
    "\n",
    "\"\"\" plot the series for the top k channels\n",
    "\"\"\"\n",
    "# define number of rows and columns for subplots\n",
    "# nrow = 3\n",
    "# ncol = 2\n",
    "# for condition, feature_to_pc in avg_condition_to_features.items():\n",
    "#     print(f\"{condition}\")\n",
    "#     ser_list = get_eeg_pearson_correlation_series_all_blocks(feature_to_pc, channel_names, k=20)\n",
    "#     plot_series(nrow, ncol, ser_list)\n",
    "\n",
    "\"\"\" plot the topography for eeg\n",
    "\"\"\"\n",
    "# for condition, feature_to_pc in avg_condition_to_features.items():\n",
    "#     plot_eeg_topomap_all_blocks(condition, feature_to_pc)\n",
    "#     for f in list(EEG_BANDS.keys()):\n",
    "#         plot_eeg_topomap_one_block(condition, f, feature_to_pc, all_block_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" extract features from physiological signals\n",
    "\"\"\"\n",
    "dir_name_to_features = {}\n",
    "dir_name_to_labels = {}\n",
    "channel_num = 1\n",
    "for dir_name, all_data in dir_to_data.items():\n",
    "    feature_to_value = get_all_features_by_marker(\n",
    "        all_data, marker, features, channel_num\n",
    "    )\n",
    "\n",
    "    dir_name_to_features[dir_name] = feature_to_value\n",
    "    dir_name_to_labels[dir_name] = get_all_behaviors_labels(all_data)\n",
    "\n",
    "features_to_trials = defaultdict()\n",
    "for dir_name, fv in dir_name_to_features.items():\n",
    "    for f, v in fv.items():\n",
    "        if f not in features_to_trials:\n",
    "            features_to_trials[f] = defaultdict()\n",
    "        if dir_name not in features_to_trials[f]:\n",
    "            features_to_trials[f][dir_name] = defaultdict()\n",
    "\n",
    "        features_to_trials[f][dir_name] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cfc36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plot the scattor for physiological signals\n",
    "\"\"\"\n",
    "# plot_scatter_by_marker(marker, features_to_trials, dir_name_to_labels, channel_names[channel_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y values to categorical values\n",
    "name_to_transformed = get_tranformed_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c04220",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = \"attention\"\n",
    "y_transformed = name_to_transformed[label_name]\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "param_grid = {\"classifier__C\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]}\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"classifier\", clf)])\n",
    "gscv = GridSearchCV(pipe, param_grid, cv=gkf, n_jobs=16)\n",
    "gscv.fit(all_features, y_transformed, groups=group_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gscv.best_estimator_\n",
    "print(f\"best model: {best_model}\")\n",
    "print(gscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BP feature shape (13, 11)\n",
    "# ECG feature shape (13, 11) 22\n",
    "# EEG feature shape (13, 1408) 1430\n",
    "# EGG feature shape (13, 11) 1441\n",
    "# EMG feature shape (13, 22) 1463\n",
    "# EOG feature shape (13, 22) 1485\n",
    "# GSR feature shape (13, 11) 1496\n",
    "# Resp feature shape (13, 11) 1507\n",
    "# TREV feature shape (13, 11) 1518\n",
    "# features = (mean(data),std(data),ptp(data),var(data),minim(data),maxim(data),\n",
    "#                           mean_square(data),rms(data),abs_diffs_signal(data),\n",
    "#                           skewness(data),kurtosis(data))\n",
    "# EEG spetral features shape (13, 768) 2286-768=\n",
    "\n",
    "STATS_NAME = [\n",
    "    \"mean\",\n",
    "    \"std\",\n",
    "    \"ptp\",\n",
    "    \"var\",\n",
    "    \"minim\",\n",
    "    \"maxim\",\n",
    "    \"mean_square\",\n",
    "    \"rms\",\n",
    "    \"abs_diffs_signal\",\n",
    "    \"skewness\",\n",
    "    \"kurtosis\",\n",
    "]\n",
    "EEG_BANDS = [\"Delta\", \"Theta\", \"Alpha\", \"Beta1\", \"Beta2\", \"Gamma\"]\n",
    "\n",
    "\n",
    "def get_stats_name(i) -> str:\n",
    "    pos = i % 11\n",
    "    return STATS_NAME[pos]\n",
    "\n",
    "\n",
    "def get_feature_names(features):\n",
    "    index = []\n",
    "    for i, j in enumerate(features):\n",
    "        stat_name = get_stats_name(i)\n",
    "        if i < 11:\n",
    "            name = \"BP\"\n",
    "        elif i < 22:\n",
    "            name = \"ECG\"\n",
    "        elif i < 1430:\n",
    "            channel = int((i - 22) / 11) + 1\n",
    "            name = f\"EEG {channel}\"\n",
    "        elif i < 1441:\n",
    "            name = \"EGG\"\n",
    "        elif i < 1463:\n",
    "            channel = int((i - 1441) / 11) + 1\n",
    "            name = f\"EMG {channel}\"\n",
    "        elif i < 1485:\n",
    "            channel = int((i - 1463) / 11) + 1\n",
    "            name = f\"EOG {channel}\"\n",
    "        elif i < 1496:\n",
    "            name = \"GSR\"\n",
    "        elif i < 1507:\n",
    "            name = \"Resp\"\n",
    "        elif i < 1518:\n",
    "            name = \"TREV\"\n",
    "        else:\n",
    "            channel = int((i - 1518) / 6) + 1\n",
    "            name = f\"EEG {channel}\"\n",
    "            stat_name = EEG_BANDS[(i - 1518) % 6]\n",
    "\n",
    "        index.append(f\"{name} {stat_name}\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcadd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume bigger coefficents has more contribution to the model\n",
    "# but have to be sure that the features has THE SAME SCALE otherwise this assumption is not correct.\n",
    "importance = best_model[\"classifier\"].coef_[0]\n",
    "\n",
    "feat_importances = pd.Series(importance, index=get_feature_names(importance))\n",
    "feat_importances.nlargest(10).plot(\n",
    "    kind=\"barh\", title=f\"{label_name} Feature Importance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149650e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Randoom forest (not suitable as the data is too small and the model will overfit easily)\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import matplotlib\n",
    "\n",
    "# clf = RandomForestClassifier()\n",
    "# clf = clf.fit(all_features,y_transformed)\n",
    "# clf.feature_importances_\n",
    "# feat_importances = pd.Series(clf.feature_importances_, index=get_feature_names(clf.feature_importances_))\n",
    "# feat_importances.nlargest(20).plot(kind='barh',title = 'Feature Importance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a61a162a2d9e3da5a6990904193197d11ef3e5d95aabe700de904d0b7a2b7b7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
