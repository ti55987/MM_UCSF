{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "import cebra\n",
    "from cebra import CEBRA\n",
    "import seaborn as sns\n",
    "from dataframe.csv_utils import (\n",
    "    load_data_from_csv,\n",
    ")\n",
    "\n",
    "from labels import get_behavioral_labels\n",
    "\n",
    "from constants import SUEJECT_BATCHES, SORTED_BLOCK_NAMES, V_COLOR_MAP, AUDIO_BLOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import labels\n",
    "# importlib.reload(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import (\n",
    "    load_data_from_dir,\n",
    ")\n",
    "\n",
    "subject_list = [2001, 2003, 2017, 2026, 2028, 2033,  2037, 2041]\n",
    "subj_to_data = {}\n",
    "data_dir = '../CleandDataV2/'\n",
    "for subj in subject_list:\n",
    "    dir_name = data_dir + str(subj)\n",
    "    all_data = load_data_from_dir(dir_name)\n",
    "    subj_to_data[subj] = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features.psd import welch_bandpower\n",
    "from feature_extraction import EEG_BANDS, Feature\n",
    "\n",
    "\n",
    "def get_psd(trial_data, srate, band):\n",
    "    low, high = band\n",
    "    freqs, psd = welch_bandpower(trial_data, srate, None, 2)\n",
    "\n",
    "    # Find closest indices of band in frequency vector\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "\n",
    "    return psd[idx_band]\n",
    "\n",
    "\n",
    "def get_psd_by_channel(block_data, marker, channel_type: str, feature: Feature):\n",
    "    psd_data = []\n",
    "    time_series_data = block_data.get_all_data()[marker]\n",
    "\n",
    "    # loop through all trials: time -> frequency\n",
    "    for t in range(time_series_data.shape[2]):\n",
    "        all_channel_psd = []\n",
    "        for i, c in enumerate(block_data.get_chanlocs(marker)):\n",
    "            if not c.startswith(channel_type):\n",
    "                continue\n",
    "\n",
    "            data = time_series_data[i]\n",
    "            psd = get_psd(data[:, t], block_data.get_srate(marker), EEG_BANDS[feature])\n",
    "            all_channel_psd = (\n",
    "                np.hstack((all_channel_psd, psd)) if len(all_channel_psd) > 0 else psd\n",
    "            )\n",
    "\n",
    "        psd_data = (\n",
    "            np.vstack((psd_data, all_channel_psd))\n",
    "            if len(psd_data) > 0\n",
    "            else all_channel_psd\n",
    "        )\n",
    "\n",
    "    return psd_data\n",
    "\n",
    "\n",
    "def get_time_series_data_by_channel(block_data, marker, channel_type: str):\n",
    "    channel_data = []\n",
    "    time_series_data = block_data.get_all_data()[marker]\n",
    "\n",
    "    # loop through all trials: time\n",
    "    for t in range(time_series_data.shape[2]):\n",
    "        all_channel_data = []\n",
    "        for i, c in enumerate(block_data.get_chanlocs(marker)):\n",
    "            if not c.startswith(channel_type):\n",
    "                continue\n",
    "            # data is in the shape of (12288, 13)\n",
    "            data = np.array(time_series_data[i][:, t])\n",
    "            all_channel_data = (\n",
    "                np.vstack((all_channel_data, data))\n",
    "                if len(all_channel_data) > 0\n",
    "                else data\n",
    "            )\n",
    "\n",
    "        all_channel_data = np.swapaxes(all_channel_data, 0, -1)\n",
    "\n",
    "        channel_data = (\n",
    "            np.vstack((channel_data, all_channel_data))\n",
    "            if len(channel_data) > 0\n",
    "            else all_channel_data\n",
    "        )\n",
    "\n",
    "    return channel_data\n",
    "\n",
    "\n",
    "def get_block_features(blocks, subject_data, marker, channel, feature):\n",
    "    raw_data = []\n",
    "\n",
    "    for b in blocks:\n",
    "        block_data = subject_data[b]\n",
    "        psd_data = get_psd_by_channel(block_data, marker, channel, feature)\n",
    "        raw_data = np.vstack((psd_data, raw_data)) if len(raw_data) > 0 else psd_data\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def get_categorical_labels(blocks, subject_data):\n",
    "    behavioral_labels = []\n",
    "\n",
    "    for b in blocks:\n",
    "        block_data = subject_data[b]\n",
    "        v_label = block_data.get_labels()\n",
    "        a_label = block_data.get_labels(\"arousal\")\n",
    "\n",
    "        labels = [\n",
    "            get_behavioral_labels(v_label[i], a_label[i]) for i in range(len(v_label))\n",
    "        ]\n",
    "        behavioral_labels.extend(labels)\n",
    "\n",
    "    return behavioral_labels\n",
    "\n",
    "\n",
    "def get_raw_labels(blocks, subject_data):\n",
    "    v_labels = []\n",
    "    a_labels = []\n",
    "\n",
    "    for b in blocks:\n",
    "        block_data = subject_data[b]\n",
    "\n",
    "        v_labels.extend(block_data.get_labels().flatten())\n",
    "        a_labels.extend(block_data.get_labels(\"arousal\").flatten())\n",
    "\n",
    "    return v_labels, a_labels\n",
    "\n",
    "\n",
    "def get_block_time_series_features(blocks, subject_data, marker, channel):\n",
    "    raw_data = []\n",
    "    behavioral_labels = []\n",
    "\n",
    "    for b in blocks:\n",
    "        block_data = subject_data[b]\n",
    "        psd_data = get_time_series_data_by_channel(block_data, marker, channel)\n",
    "\n",
    "        v_label = block_data.get_labels()\n",
    "        a_label = block_data.get_labels(\"arousal\")\n",
    "        # (TODO) extend the labels to match the time series dimention\n",
    "        labels = [\n",
    "            get_behavioral_labels(v_label[i], a_label[i]) for i in range(len(v_label))\n",
    "        ]\n",
    "        behavioral_labels.extend(labels)\n",
    "\n",
    "        raw_data = np.vstack((psd_data, raw_data)) if len(raw_data) > 0 else psd_data\n",
    "\n",
    "    return raw_data, behavioral_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = \"EEG\"\n",
    "# [2001, 2003, 2017, 2026, 2028, 2033,  2037, 2041]\n",
    "subj = 2033\n",
    "subject_data = subj_to_data[subj]\n",
    "use_time_series = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_feature_to_time_series_data = {}\n",
    "channel_feature_to_data = {\"A\": {}, \"B\": {}, \"C\": {}, \"D\": {}}\n",
    "if use_time_series:\n",
    "    for c in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        channel_feature_to_time_series_data[c], _ = get_block_time_series_features(AUDIO_BLOCKS, subject_data, marker, c)\n",
    "else:\n",
    "    for c in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        for f in EEG_BANDS.keys():\n",
    "            raw_data = get_block_features(\n",
    "                AUDIO_BLOCKS, subject_data, marker, c, f\n",
    "            )\n",
    "            channel_feature_to_data[c][f] = raw_data\n",
    "\n",
    "    valence_labels, arousal_labels = get_raw_labels(AUDIO_BLOCKS, subject_data)\n",
    "    behavioral_labels = get_categorical_labels(AUDIO_BLOCKS, subject_data)\n",
    "    print(valence_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EEG average power spectral features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"eeg_features2\"\n",
    "result = load_data_from_csv(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = result['Subject'].unique()\n",
    "\n",
    "all_blocks = []\n",
    "for b in SORTED_BLOCK_NAMES:\n",
    "    all_blocks.extend([b] * 13)\n",
    "\n",
    "\n",
    "result[\"condition\"] = all_blocks * len(subjects)\n",
    "mask = result[\"condition\"].isin(AUDIO_BLOCKS)\n",
    "audio_only = result[mask]\n",
    "audio_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First subject only\n",
    "subject_data = audio_only[audio_only['Subject'].isin([2000])]\n",
    "neural_data = subject_data.drop(columns=['Valence', 'Arousal', 'Attention', 'Subject', 'condition'])\n",
    "v_label = subject_data['Valence']\n",
    "a_label = subject_data['Arousal']\n",
    "\n",
    "behavioral_labels = [get_behavioral_labels(v_label[i], a_label[i]) for i in range(len(v_label))]\n",
    "print(neural_data.shape, len(behavioral_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(neural_data, out_dim, num_hidden_units, behavioral_labels, max_iterations: int=10, max_adapt_iterations: int=10):\n",
    "    single_cebra_model = CEBRA(\n",
    "        # model_architecture = \"offset10-model\",\n",
    "        batch_size=512,\n",
    "        output_dimension=out_dim,\n",
    "        max_iterations=max_iterations,\n",
    "        num_hidden_units=num_hidden_units,\n",
    "        max_adapt_iterations=max_adapt_iterations,\n",
    "    )\n",
    "\n",
    "    if behavioral_labels is None:\n",
    "        single_cebra_model.fit(neural_data)\n",
    "    else:\n",
    "        single_cebra_model.fit(neural_data, behavioral_labels)\n",
    "    # cebra.plot_loss(single_cebra_model)\n",
    "    return single_cebra_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize time embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = len(channel_feature_to_time_series_data)\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(ncols * 5, nrows * 5),\n",
    "    subplot_kw=dict(projection=\"3d\"),\n",
    ")\n",
    "fig.suptitle(\n",
    "    f\"Subject {subj}: {marker} time latent embedding in audio condition\",\n",
    "    fontsize=15,\n",
    ")\n",
    "\n",
    "idx = 0\n",
    "loss_data = {}\n",
    "for channel, data in channel_feature_to_time_series_data.items():\n",
    "    output_dim = 16\n",
    "    max_hidden_units = 256\n",
    "    single_cebra_model = model_fit(\n",
    "        neural_data, output_dim, max_hidden_units, None, 150, 150\n",
    "    )\n",
    "    loss_data[channel] = single_cebra_model.state_dict_[\"loss\"]\n",
    "    embedding = single_cebra_model.transform(neural_data)\n",
    "\n",
    "    ax = cebra.plot_embedding(\n",
    "        embedding,\n",
    "        embedding_labels='time',\n",
    "        title=f\"{channel} Channels\",\n",
    "        markersize=5,\n",
    "        alpha=0.6,\n",
    "        ax=axes.flat[idx],\n",
    "    )\n",
    "    idx += 1\n",
    "\n",
    "#plt.savefig(f\"results/cebra/cebra_{subj}_eeg_time_series_channel_{channel}_O{output_dim}H{max_hidden_units}.png\")\n",
    "loss_data = pd.DataFrame(loss_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=loss_data)\n",
    "plt.savefig(f\"results/cebra/cebra_{subj}_eeg_time_series_infoloss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cebra.plot_loss(single_cebra_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "label_selector = widgets.Dropdown(\n",
    "    options=[\"valence\", \"hybrid\", \"arousal\"],\n",
    "    value='valence',\n",
    "    description='label_type:',\n",
    "    disabled=False,\n",
    ")\n",
    "label_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_MAP = {\n",
    "    \"hvha\": 0,\n",
    "    \"hvla\": 1,\n",
    "    \"nvha\": 2,\n",
    "    \"nvla\": 3,\n",
    "    \"lvha\": \"blue\",\n",
    "    \"lvla\": \"steelblue\",\n",
    "}\n",
    "\n",
    "label_type = label_selector.value\n",
    "cmap = [V_COLOR_MAP[l] for l in behavioral_labels]\n",
    "\n",
    "if label_type == \"valence\":\n",
    "    labels = valence_labels\n",
    "elif label_type == \"arousal\":\n",
    "    labels = arousal_labels\n",
    "else:\n",
    "    labels = [IDX_MAP[l] for l in behavioral_labels]\n",
    "\n",
    "loss_data = {\"A\": {}, \"B\": {}, \"C\": {}, \"D\": {}}\n",
    "for channel, feature_to_data in channel_feature_to_data.items():\n",
    "    nrows = 2\n",
    "    ncols = int(len(feature_to_data) / 2)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2,\n",
    "        ncols=ncols,\n",
    "        figsize=(ncols * 5, nrows * 5),\n",
    "        subplot_kw=dict(projection=\"3d\"),\n",
    "    )\n",
    "\n",
    "    idx = 0\n",
    "    output_dim = 8\n",
    "    max_hidden_units = 256\n",
    "\n",
    "    for f, neural_data in feature_to_data.items():\n",
    "        single_cebra_model = model_fit(\n",
    "            neural_data, output_dim, max_hidden_units, np.array(labels)\n",
    "        )\n",
    "        loss_data[channel][f.name] = single_cebra_model.state_dict_[\"loss\"]\n",
    "\n",
    "        # plot embedding\n",
    "        embedding = single_cebra_model.transform(neural_data)\n",
    "        ax = cebra.plot_embedding(\n",
    "            embedding,\n",
    "            embedding_labels=np.array(cmap),\n",
    "            title=f\"{channel}:{f.name}\",\n",
    "            ax=axes.flat[idx],\n",
    "            markersize=5,\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        idx += 1\n",
    "\n",
    "    plt.legend(\n",
    "        handles=[\n",
    "            mpatches.Patch(color=\"red\", label=\"hvha\", alpha=0.6),\n",
    "            mpatches.Patch(color=\"magenta\", label=\"hvla\", alpha=0.6),\n",
    "            mpatches.Patch(color=\"green\", label=\"nvha\", alpha=0.6),\n",
    "            mpatches.Patch(color=\"olive\", label=\"nvla\", alpha=0.6),\n",
    "        ],\n",
    "        loc=\"lower right\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Subject {subj}: {marker} {label_type} latent embedding in audio condition\",\n",
    "        fontsize=15,\n",
    "    )\n",
    "\n",
    "    plt.savefig(f\"results/cebra/{label_type}_{subj}_eeg_bands_channel_{channel}_O{output_dim}H{max_hidden_units}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = len(loss_data)\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "     sharey=True,\n",
    "    figsize=(ncols * 5, nrows * 5),\n",
    ")\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Subject {subj}: {marker} {label_type} InfoNCE loss in audio condition\",\n",
    "    fontsize=15,\n",
    ")\n",
    "       \n",
    "for c, ax in zip(loss_data.keys(), axes.flatten()):\n",
    "    df = pd.DataFrame(loss_data[c])\n",
    "    sns.lineplot(data=df, ax=ax)\n",
    "    ax.set_title('channel:' + c)\n",
    "    ax.set_ylabel('InfoNCE Loss')\n",
    "    ax.set_xlabel('Steps')\n",
    "plt.savefig(f\"results/cebra/{label_type}_{subj}_eeg_bands_channel_loss_{channel}_O{output_dim}H{max_hidden_units}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = {'InfoNCE Loss': [], 'band': [], 'channel': [], 'Steps': []}\n",
    "for c , f_to_data in loss_data.items():\n",
    "    for f, data in f_to_data.items():\n",
    "        loss_dict['InfoNCE Loss'].extend(np.array(data))\n",
    "        loss_dict['Steps'].extend(np.arange(0, len(data), dtype=int))\n",
    "        loss_dict['band'].extend([f]*len(data))\n",
    "        loss_dict['channel'].extend([c]*len(data))\n",
    "\n",
    "loss_dict = pd.DataFrame(loss_dict)\n",
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=loss_dict, y='InfoNCE Loss', x='Steps', hue='band', style='channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the parameters, either variable or fixed\n",
    "params_grid = dict(\n",
    "    output_dimension=[6, 8],\n",
    "    learning_rate=[3e-4],\n",
    "    max_iterations=10,\n",
    "    num_hidden_units=[32, 64, 128, 256],\n",
    "    max_adapt_iterations=10,\n",
    "    temperature_mode=\"auto\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# 2. Define the datasets to iterate over\n",
    "datasets = {\n",
    "    \"neural_data\": channel_feature_to_data[\"C\"][Feature.THETA],\n",
    "}\n",
    "\n",
    "# 3. Create and fit the grid search to your data\n",
    "grid_search = cebra.grid_search.GridSearch()\n",
    "grid_search = grid_search.fit_models(datasets=datasets, params=params_grid, models_dir=\"saved_models\")\n",
    "\n",
    "# 4. Get the results\n",
    "df_results = grid_search.get_df_results(models_dir=\"saved_models\")\n",
    "# 5. Get the best model for a given dataset\n",
    "best_model, best_model_name = grid_search.get_best_model(dataset_name=\"neural_data\", models_dir=\"saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "\n",
    "ax1 = cebra.plot_embedding(embedding, embedding_labels=np.array(cmap), idx_order=(1,2,3), title=\"Latents: (1,2,3)\", ax=ax1, markersize=5, alpha=0.6)\n",
    "ax2 = cebra.plot_embedding(embedding, embedding_labels=np.array(cmap), idx_order=(4,5,6), title=\"Latents: (4,5,6)\", ax=ax2, markersize=5, alpha=0.6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
