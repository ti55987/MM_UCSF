{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataframe.csv_utils import (\n",
    "    load_data_from_csv,\n",
    "    get_filtered_data,\n",
    ")\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.decomposition import PCA  # Principal Component Analysis\n",
    "from sklearn.manifold import TSNE  # T-Distributed Stochastic Neighbor Embedding\n",
    "\n",
    "# plotly imports\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode\n",
    "\n",
    "SORTED_BLOCK_NAMES = [\n",
    "    \"audio_hvha\",\n",
    "    \"audio_hvla\",\n",
    "    \"audio_nvha\",\n",
    "    \"audio_nvla\",\n",
    "    \"breath_hvha\",\n",
    "    \"breath_hvla\",\n",
    "    \"breath_nvha\",\n",
    "    \"breath_nvla\",\n",
    "    \"meditation\",\n",
    "    \"mind wandering\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    load features from csv\n",
    "\"\"\"\n",
    "dir_name = \"eeg_features2\"\n",
    "result = load_data_from_csv(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed so we can display plotly plots properly\n",
    "init_notebook_mode(connected=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.offline import plot\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "COLOR_MAP = {\n",
    "    \"audio_hvha\": \"red\",\n",
    "    \"audio_hvla\": \"magenta\",\n",
    "    \"audio_nvha\": \"green\",\n",
    "    \"audio_nvla\": \"olive\",\n",
    "}\n",
    "V_COLOR_MAP = {\n",
    "    \"hvha\": \"red\",\n",
    "    \"hvla\": \"magenta\",\n",
    "    \"nvha\": \"green\",\n",
    "    \"nvla\": \"olive\",\n",
    "    \"lvha\": \"blue\",\n",
    "    \"lvla\": \"steelblue\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_filter_pattern(feature: str = \"ALL\", channel: str = \"\") -> str:\n",
    "    prefix = \"\"\n",
    "    if len(channel) > 0:\n",
    "        prefix = f\"^{channel}\"\n",
    "\n",
    "    # get_other feature_pattern = \".*(?<!GAMMA|BETA2|BETA1|ALPHA|THETA|DELTA)$\"\n",
    "    if feature == \"ALL\":\n",
    "        return f\"{prefix}.*(?<!sdf)$\"\n",
    "\n",
    "    return f\"{prefix}.*{feature}$\"  # f\"^{feature}.*(?<!BETA2)$\"\n",
    "\n",
    "\n",
    "def get_processed_feature(\n",
    "    feature: str, channel: str, result: pd.DataFrame, subjects: list = []\n",
    "):\n",
    "    pattern = get_filter_pattern(feature, channel)\n",
    "    normalized_eeg_features, filtered_result, label_list = get_filtered_data(\n",
    "        result, subjects, pattern\n",
    "    )\n",
    "    print(\"filtered\", normalized_eeg_features.shape)\n",
    "    return normalized_eeg_features, filtered_result\n",
    "\n",
    "\n",
    "def get_umap(\n",
    "    feature: str,\n",
    "    channel: str,\n",
    "    result: pd.DataFrame,\n",
    "    all_blocks: list,\n",
    "    subjects: list = [],\n",
    "):\n",
    "    pattern = get_filter_pattern(feature, channel)\n",
    "    normalized_eeg_features, filtered_result, label_list = get_filtered_data(\n",
    "        result, subjects, pattern\n",
    "    )\n",
    "    print(normalized_eeg_features.shape)\n",
    "\n",
    "    # Run UMAP\n",
    "    umap2d = UMAP(n_components=2, init=\"random\", random_state=0)\n",
    "    proj_2d = pd.DataFrame(umap2d.fit_transform(normalized_eeg_features))\n",
    "\n",
    "    # Concatanate the umap points and original data\n",
    "    filtered_result = filtered_result.reset_index()\n",
    "    filtered_result[\"condition\"] = all_blocks * len(subjects)\n",
    "    proj_2d.columns = [\"C1_2d\", \"C2_2d\"]\n",
    "    return pd.concat([filtered_result, proj_2d], axis=1, join=\"inner\")\n",
    "\n",
    "\n",
    "def get_pca(feature: str, channel: str, result: pd.DataFrame, subjects: list = []):\n",
    "    normalized_eeg_features, filtered_result = get_processed_feature(\n",
    "        feature, channel, result, subjects\n",
    "    )\n",
    "\n",
    "    # Run PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    proj_2d = pd.DataFrame(pca.fit_transform(normalized_eeg_features))\n",
    "\n",
    "    # Concatanate the umap points and original data\n",
    "    filtered_result = filtered_result.reset_index()\n",
    "    proj_2d.columns = [\"C1_2d\", \"C2_2d\"]\n",
    "    return pd.concat([filtered_result[\"Subject\"], proj_2d], axis=1, join=\"inner\")\n",
    "\n",
    "\n",
    "def show_result(\n",
    "    feature: str,\n",
    "    subjects: list,\n",
    "    plotX: pd.DataFrame,\n",
    "    conditions: list,\n",
    "    to_save: bool = False,\n",
    "):\n",
    "    title = f\"EEG {feature} (average spectral power per trial)\"\n",
    "\n",
    "    figures = []\n",
    "    for subj in subjects:\n",
    "        df = plotX[plotX[\"Subject\"] == subj]\n",
    "        figures.append(\n",
    "            px.scatter(\n",
    "                df,\n",
    "                x=\"C1_2d\",\n",
    "                y=\"C2_2d\",\n",
    "                color=df[\"condition\"],\n",
    "                color_discrete_map=V_COLOR_MAP,\n",
    "                opacity=0.6,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=int((len(figures) / 2) + (len(figures) % 2)),\n",
    "        cols=2,\n",
    "        subplot_titles=subjects,\n",
    "        horizontal_spacing=0.1,\n",
    "        vertical_spacing=0.05,\n",
    "    )\n",
    "\n",
    "    for i, figure in enumerate(figures):\n",
    "        for trace in range(len(figure[\"data\"])):\n",
    "            figure[\"data\"][trace].update()\n",
    "            fig.append_trace(\n",
    "                figure[\"data\"][trace], row=int(i / 2 + 1), col=int(i % 2 + 1)\n",
    "            )\n",
    "\n",
    "    # Deduplicate the legends\n",
    "    names = set()\n",
    "    fig.for_each_trace(\n",
    "        lambda trace: trace.update(showlegend=False)\n",
    "        if (trace.name in names)\n",
    "        else names.add(trace.name)\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1000, width=1000, title_text=title, margin=dict(r=0, b=0, l=0)\n",
    "    )\n",
    "\n",
    "    name = \",\".join(str(s) for s in subjects)\n",
    "    filename = f\"results/{feature}_{name}.png\"\n",
    "    if to_save:\n",
    "        fig.write_image(filename)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def show_pca_matrix(\n",
    "    band: str,\n",
    "    channel: str,\n",
    "    subject: int,\n",
    "    result: pd.DataFrame,\n",
    "    all_blocks: list,\n",
    "    n_components: int,\n",
    "    to_save: bool = False,\n",
    "):\n",
    "    normalized_eeg_features, _ = get_processed_feature(band, channel, result, [subject])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    proj_2d = pd.DataFrame(pca.fit_transform(normalized_eeg_features))\n",
    "    labels = {\n",
    "        str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "        for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "    }\n",
    "\n",
    "    identifier = str(subject) + f\"_{band}_{channel}_channel\"\n",
    "    total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "    fig = px.scatter_matrix(\n",
    "        proj_2d,\n",
    "        labels=labels,\n",
    "        dimensions=range(len(labels)),\n",
    "        color=all_blocks,\n",
    "        color_discrete_map=V_COLOR_MAP,\n",
    "        opacity=0.6,\n",
    "        title=f\"{identifier} Total Explained Variance: {total_var:.2f}%\",\n",
    "    )\n",
    "    fig.update_traces(diagonal_visible=False)\n",
    "\n",
    "    # Deduplicate the legends\n",
    "    names = set()\n",
    "    fig.for_each_trace(\n",
    "        lambda trace: trace.update(showlegend=False)\n",
    "        if (trace.name in names)\n",
    "        else names.add(trace.name)\n",
    "    )\n",
    "\n",
    "    fig.update_layout(height=1000, width=1000, margin=dict(r=0, b=0, l=0))\n",
    "\n",
    "    filename = f\"results/{identifier}_PCA_breakdown.png\"\n",
    "    if to_save:\n",
    "        fig.write_image(filename)\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e911ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOUR_CONDITIONS = [\"audio\"] * 4 + [\"breath\"] * 4 + [\"meditation\", \"wandering\"]\n",
    "VALENCE_CONDITIONS = (\n",
    "    [\"h_valence\"] * 2\n",
    "    + [\"n_valence\"] * 2\n",
    "    + [\"h_valence\"] * 2\n",
    "    + [\"n_valence\"] * 2\n",
    "    + [\"meditation\", \"wandering\"]\n",
    ")\n",
    "\n",
    "AROUSAL_CONDITIONS = [\"h_arousal\", \"l_arousal\"] * 4 + [\"meditation\", \"wandering\"]\n",
    "\n",
    "name_to_batch = {\n",
    "    \"f\": [2017, 2018, 2020, 2024, 2025, 2026],\n",
    "    \"s\": [2028, 2029, 2031, 2032, 2033, 2035],\n",
    "    \"t\": [2036, 2039, 2040, 2041, 2042, 2043, 2044, 2045],\n",
    "}\n",
    "\n",
    "subjects = name_to_batch[\"s\"]\n",
    "all_blocks = []\n",
    "for b in SORTED_BLOCK_NAMES:\n",
    "    all_blocks.extend([b] * 13)\n",
    "\n",
    "result[\"condition\"] = all_blocks * len(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_blocks = [\"audio_hvha\", \"audio_hvla\", \"audio_nvha\", \"audio_nvla\"]\n",
    "\n",
    "mask = result[\"condition\"].isin(audio_blocks)\n",
    "audio_only = result[mask].drop(\"condition\", axis=1)\n",
    "\n",
    "# True label as conditions\n",
    "# all_blocks = []\n",
    "# for b in audio_blocks:\n",
    "#     all_blocks.extend([b] * 13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f65d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True label as user ratings\n",
    "v_r = audio_only[\"Valence\"].tolist()\n",
    "a_r = audio_only[\"Arousal\"].tolist()\n",
    "all_blocks = []\n",
    "for i in range(audio_only.shape[0]):\n",
    "    valence = \"hv\"\n",
    "    if v_r[i] < 0.4:\n",
    "        valence = \"lv\"\n",
    "    elif v_r[i] < 0.6:\n",
    "        valence = \"nv\"\n",
    "\n",
    "    arousal = \"ha\" if a_r[i] > 0.5 else \"la\"\n",
    "    all_blocks.append(valence + arousal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ed303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://plotly.com/python/pca-visualization/\n",
    "for band in [\"ALL\", \"DELTA\", \"THETA\", \"ALPHA\", \"BETA1\", \"BETA2\", \"GAMMA\"]:\n",
    "    for ch in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        plotX = get_pca(band, ch, audio_only, subjects)\n",
    "        feature = f\"rating_audio_{band}_{ch}_channel_PCA\"\n",
    "        plotX[\"condition\"] = all_blocks\n",
    "        show_result(feature, subjects, plotX, all_blocks, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ca2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 6\n",
    "audio_only[\"condition\"] = all_blocks\n",
    "for s in subjects:\n",
    "    conditions = audio_only[audio_only[\"Subject\"] == s][\"condition\"]\n",
    "    show_pca_matrix(\n",
    "        \"ALPHA\",\n",
    "        \"C\",\n",
    "        s,\n",
    "        audio_only.drop(\"condition\", axis=1),\n",
    "        conditions,\n",
    "        n_components,\n",
    "        False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72944ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import rand_score\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def do_hierarchical_clustering(normalized_eeg_features):\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    import time\n",
    "\n",
    "    print(\"Compute unstructured hierarchical clustering...\")\n",
    "    st = time.time()\n",
    "    ward = AgglomerativeClustering(n_clusters=4, linkage=\"ward\").fit(\n",
    "        normalized_eeg_features\n",
    "    )\n",
    "    elapsed_time = time.time() - st\n",
    "    print(f\"Elapsed time: {elapsed_time:.2f}s\")\n",
    "    #     print(f\"Number of points: {ward.labels_.size}\")\n",
    "    return ward.labels_\n",
    "\n",
    "\n",
    "def do_kmeans(normalized_eeg_features):\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    kmeans = KMeans(init=\"k-means++\", n_clusters=4, n_init=2)\n",
    "    kmeans.fit(normalized_eeg_features)\n",
    "    # Find which cluster each data-point belongs to\n",
    "    return kmeans.predict(normalized_eeg_features)\n",
    "\n",
    "\n",
    "heatmap_data = []\n",
    "s = 2031\n",
    "for ch in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    scores = defaultdict()\n",
    "    for band in [\"ALL\", \"DELTA\", \"THETA\", \"ALPHA\", \"BETA1\", \"BETA2\", \"GAMMA\"]:\n",
    "        # reference https://www.mikulskibartosz.name/pca-how-to-choose-the-number-of-components/\n",
    "        normalized_eeg_features, _ = get_processed_feature(\n",
    "            band, ch, audio_only.drop(\"condition\", axis=1), [s]\n",
    "        )\n",
    "        reduced_data = PCA(n_components=0.95).fit_transform(normalized_eeg_features)\n",
    "        normalized_eeg_features = pd.DataFrame(reduced_data)\n",
    "        # print('components', normalized_eeg_features.shape)\n",
    "\n",
    "        true_labels = audio_only[audio_only[\"Subject\"] == s][\"condition\"]\n",
    "        score = rand_score(true_labels, do_kmeans(normalized_eeg_features))\n",
    "        # print(f'{band}:{ch}:{score}')\n",
    "        scores[band] = score\n",
    "    heatmap_data.append(scores)\n",
    "heatmap_data = pd.DataFrame(heatmap_data, index=[\"A\", \"B\", \"C\", \"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.set_title(f\"{s} subject Kmeans clustering rand index\")\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", linewidths=0.5, ax=ax, cmap=\"crest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataframe.visualization import pca_2d\n",
    "\n",
    "# Add the cluster vector to our DataFrame, X\n",
    "normalized_eeg_features[\"Cluster\"] = clusters\n",
    "# normalized_eeg_features['Block'] = all_blocks\n",
    "\n",
    "pca_2d(\n",
    "    normalized_eeg_features,\n",
    "    4,\n",
    "    [\n",
    "        \"rgba(255, 128, 255, 0.8)\",\n",
    "        \"rgba(255, 128, 2, 0.8)\",\n",
    "        \"rgba(0, 255, 200, 0.8)\",\n",
    "        \"rgba(0, 128, 200, 0.8)\",\n",
    "    ],\n",
    "    \"hello\",\n",
    "    False,\n",
    "    mode=\"markers\",\n",
    "    textfont=dict(size=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf95317",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [\"ALL\", \"DELTA\", \"THETA\", \"ALPHA\", \"BETA1\", \"BETA2\", \"GAMMA\"]:\n",
    "    plotX = get_umap(f, result, all_blocks, subjects)\n",
    "    feature = f\"{f}_UMAP\"\n",
    "    show_result(feature, subjects, plotX, condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc646a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.mixture import GaussianMixture\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# gmm = GaussianMixture(n_components=2).fit(normalized_eeg_features)\n",
    "# clusters = gmm.predict(normalized_eeg_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617386cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn import metrics\n",
    "\n",
    "# db = DBSCAN(eps=7, min_samples=10).fit(normalized_eeg_features)\n",
    "# clusters = db.labels_\n",
    "\n",
    "# # Number of clusters in labels, ignoring noise if present.\n",
    "# n_clusters_ = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "# n_noise_ = list(clusters).count(-1)\n",
    "\n",
    "# print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "# print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "# clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization via t-SNE\n",
    "# Set our perplexity\n",
    "perplexity = 50\n",
    "# T-SNE with one dimension\n",
    "tsne_1d = TSNE(n_components=1, perplexity=perplexity)\n",
    "\n",
    "# T-SNE with two dimensions\n",
    "tsne_2d = TSNE(n_components=2, perplexity=perplexity)\n",
    "\n",
    "# T-SNE with three dimensions\n",
    "tsne_3d = TSNE(n_components=3, perplexity=perplexity)\n",
    "# This DataFrame holds a single dimension,built by T-SNE\n",
    "s_1d = pd.DataFrame(\n",
    "    tsne_1d.fit_transform(normalized_eeg_features.drop([\"Cluster\"], axis=1))\n",
    ")\n",
    "\n",
    "# This DataFrame contains two dimensions, built by T-SNE\n",
    "s_2d = pd.DataFrame(\n",
    "    tsne_2d.fit_transform(normalized_eeg_features.drop([\"Cluster\"], axis=1))\n",
    ")\n",
    "\n",
    "# And this DataFrame contains three dimensions, built by T-SNE\n",
    "s_3d = pd.DataFrame(\n",
    "    tsne_3d.fit_transform(normalized_eeg_features.drop([\"Cluster\"], axis=1))\n",
    ")\n",
    "\n",
    "s_1d.columns = [\"TC1_1d\"]\n",
    "\n",
    "# \"TC1_2d\" means: 'The first component of the components created for 2-D visualization, by T-SNE.'\n",
    "# And \"TC2_2d\" means: 'The second component of the components created for 2-D visualization, by T-SNE.'\n",
    "s_2d.columns = [\"TC1_2d\", \"TC2_2d\"]\n",
    "\n",
    "s_3d.columns = [\"TC1_3d\", \"TC2_3d\", \"TC3_3d\"]\n",
    "method = \"t-SNE\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MM",
   "language": "python",
   "name": "mm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
