{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d653b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from dataframe.csv_utils import (\n",
    "    load_data_from_csv,\n",
    "    get_labels_from_result,\n",
    "    get_features_from_result,\n",
    ")\n",
    "\n",
    "\n",
    "# For machine learning modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from constants import AUDIO_BLOCKS\n",
    "\n",
    "NUM_LABEL_PER_SUBJECT = 130"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c347efcf",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd940f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = 'ECG'\n",
    "data_dir = \"../CleandDataV2/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0023c7c2",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fc9d0da",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/ecg_rr_peaks.pkl', 'rb') as fp:\n",
    "    ecg_features = pickle.load(fp)\n",
    "    ecg_features = ecg_features['ecg_rr_peaks']\n",
    "\n",
    "channel_name = marker + ''\n",
    "feature_name = 'ECG_HRV_RR' #Feature.ECG_LFHF.name\n",
    "\n",
    "marker_features = []\n",
    "for data in ecg_features: #all_features\n",
    "    marker_features.append({channel_name: {feature_name: data}})\n",
    "marker_features[0][channel_name][feature_name].shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labels pkl file with slicing behavioral\n",
    "with open(\"./data/behavioral_labels.pkl\", \"rb\") as fp:\n",
    "    behavioral_labels = pickle.load(fp)\n",
    "\n",
    "sliced_valence_labels, sliced_arousal_labels, _ = (\n",
    "    behavioral_labels[\"valence_labels\"],\n",
    "    behavioral_labels[\"arousal_labels\"],\n",
    "    behavioral_labels[\"attention_labels\"],\n",
    ")\n",
    "print(len(sliced_valence_labels), len(sliced_arousal_labels[0]))\n",
    "\n",
    "# Load labels without slice\n",
    "num_slice_per_trial = 5\n",
    "valence_labels, arousal_labels, label_thresholds = [], [], []\n",
    "subject_list = []\n",
    "si = 0\n",
    "for d in os.listdir(data_dir):\n",
    "    dir_name = data_dir + d\n",
    "    if not os.path.isdir(dir_name):\n",
    "        continue    \n",
    "\n",
    "    vls, als = sliced_valence_labels[si], sliced_arousal_labels[si]\n",
    "    si += 1\n",
    "    \n",
    "    filter_indexes = np.arange(0, len(vls), num_slice_per_trial)\n",
    "    valence_labels.append(np.array(vls)[filter_indexes])\n",
    "    arousal_labels.append(np.array(als)[filter_indexes])    \n",
    "    label_thresholds.append((np.mean(vls), np.mean(als)))\n",
    "    subject_list.append(d)\n",
    "\n",
    "len(subject_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4bcc9cd",
   "metadata": {},
   "source": [
    "## Prepare labels and dataset builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resample.resample import get_consecutive_validation_indexes\n",
    "from model.dataset import DatasetBuilder\n",
    "\n",
    "n_step_trial = 3\n",
    "num_slice_per_trial = 1\n",
    "val_indexes = [\n",
    "    get_consecutive_validation_indexes(\n",
    "        len(valence_labels[0]), len(AUDIO_BLOCKS), num_slice_per_trial, i, n_step_trial\n",
    "    )\n",
    "    for i in range(1, 13, n_step_trial)\n",
    "]\n",
    "print(len(val_indexes), val_indexes)\n",
    "\n",
    "\n",
    "dataset_builder = DatasetBuilder(len(valence_labels[0]), val_indexes_group=val_indexes)\n",
    "len(valence_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc058255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers, initializers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "def get_label_category(labels, label_type, v_thred, a_thred):\n",
    "    threshold = a_thred if label_type == \"arousal\" else v_thred\n",
    "    return [0 if p < threshold else 1 for p in labels]\n",
    "\n",
    "def prepare_dataset(\n",
    "    data_dict,\n",
    "    dataset_builder,\n",
    "    labels,\n",
    "):  \n",
    "    # Combine with ecg...\n",
    "    dataset_dict = {k: {} for k in data_dict.keys()}\n",
    "    for k, feature_to_data in data_dict.items():\n",
    "        for f, fd in feature_to_data.items():\n",
    "            dataset_dict[k][f] = dataset_builder.train_test_split(fd, [], labels)\n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "def create_model(input_x: int, input_y: int=1, units=8, dropout=0.01):\n",
    "    input_layer = keras.Input(shape=(input_x, input_y))\n",
    "\n",
    "    conv_layer_1 = layers.Conv1D(\n",
    "        filters=units, kernel_size=3, dilation_rate=1, groups=1,\n",
    "        padding='causal', kernel_initializer=initializers.he_normal(seed=11)\n",
    "    )(input_layer)\n",
    "    conv_layer_1 = layers.Activation('gelu')(conv_layer_1)\n",
    "    conv_layer_1 = layers.SpatialDropout1D(rate=dropout)(conv_layer_1)\n",
    "    \n",
    "    cnn_outputs = layers.Flatten()(conv_layer_1)\n",
    "    # Dense layers\n",
    "    activation_func = 'relu'\n",
    "    outputs = layers.Dense(int(units/2), activation=activation_func)(cnn_outputs)\n",
    "    outputs = layers.Dropout(dropout)(outputs)\n",
    "    # outputs = layers.Dense(int(units/4), activation=activation_func)(outputs)\n",
    "    # outputs = Dropout(dropout2)(outputs)\n",
    "\n",
    "    output_layer = layers.Dense(2, activation=\"softmax\")(outputs)\n",
    "    optimizer = Adam(learning_rate=3e-4)\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=[\"accuracy\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131231fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "subject_accuracy_summary = {\n",
    "    \"subject\": [],\n",
    "    \"channel\": [],\n",
    "    \"feature\": [],\n",
    "    \"label_type\": [],\n",
    "    \"cv_mean_score\": [],\n",
    "}\n",
    "\n",
    "for idx in range(len(subject_list)):\n",
    "    subj = subject_list[idx]\n",
    "    print(\"decoding subject...\", subj)\n",
    "\n",
    "    v_thred, a_thred = label_thresholds[idx]\n",
    "    for lt in [\"valence\", \"arousal\"]:\n",
    "        labels = valence_labels[idx] if lt == \"valence\" else arousal_labels[idx]\n",
    "        thred = v_thred if lt == \"valence\" else a_thred\n",
    "\n",
    "        dataset_dict = prepare_dataset(\n",
    "            marker_features[idx],\n",
    "            dataset_builder,\n",
    "            labels,\n",
    "        )\n",
    "\n",
    "        for channel, feature_to_data in dataset_dict.items():\n",
    "            for f, dataset in feature_to_data.items():\n",
    "                scores = []\n",
    "                for _, (train_data, train_labels, val_data, val_labels) in enumerate(\n",
    "                    dataset\n",
    "                ):\n",
    "                    normalized_train_labels = tf.keras.utils.to_categorical(\n",
    "                        get_label_category(train_labels, lt, v_thred, a_thred),\n",
    "                        num_classes=2,\n",
    "                    )\n",
    "                    normalized_val_labels = tf.keras.utils.to_categorical(\n",
    "                        get_label_category(val_labels, lt, v_thred, a_thred),\n",
    "                        num_classes=2,\n",
    "                    )\n",
    "                    # The first data is eeg and then ecg...\n",
    "                    for idx, td in enumerate(train_data):\n",
    "                        if idx == 1:\n",
    "                            continue\n",
    "                        standard_scaler = StandardScaler()\n",
    "\n",
    "                        X_train_standard = standard_scaler.fit_transform(td)\n",
    "                        X_test_standard = standard_scaler.transform(val_data[idx])\n",
    "                        X_train_standard = np.expand_dims(X_train_standard, axis=2)\n",
    "                        X_test_standard = np.expand_dims(X_test_standard, axis=2)\n",
    "\n",
    "                        best_model = create_model(\n",
    "                            td.shape[-1], units=64, dropout=0.1\n",
    "                        )  # 0.05\n",
    "                        callbacks = [\n",
    "                            EarlyStopping(\n",
    "                                monitor=\"val_loss\",\n",
    "                                patience=35,\n",
    "                                restore_best_weights=True,\n",
    "                            )\n",
    "                        ]\n",
    "                        history = best_model.fit(\n",
    "                            x=X_train_standard,\n",
    "                            y=normalized_train_labels,\n",
    "                            validation_data=(X_test_standard, normalized_val_labels),\n",
    "                            callbacks=callbacks,\n",
    "                            epochs=200,\n",
    "                            verbose=0,\n",
    "                            batch_size=20,\n",
    "                        )\n",
    "                        # print(history.history['val_accuracy'][-1])\n",
    "                        scores.append(history.history[\"val_accuracy\"][-1])\n",
    "                print(np.mean(scores), scores)\n",
    "                subject_accuracy_summary[\"subject\"].append(subj)\n",
    "                subject_accuracy_summary[\"channel\"].append(channel)\n",
    "                subject_accuracy_summary[\"feature\"].append(f)\n",
    "                subject_accuracy_summary[\"cv_mean_score\"].append(np.mean(scores))\n",
    "                subject_accuracy_summary[\"label_type\"].append(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff37ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_accuracy_summary = pd.DataFrame(subject_accuracy_summary)\n",
    "subject_accuracy_summary[\"subject\"] = subject_accuracy_summary[\"subject\"].astype(int)\n",
    "print(subject_accuracy_summary[subject_accuracy_summary.label_type =='valence']['cv_mean_score'].mean())\n",
    "print(subject_accuracy_summary[subject_accuracy_summary.label_type =='arousal']['cv_mean_score'].mean())\n",
    "subject_accuracy_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "data = subject_accuracy_summary\n",
    "title = f\"{marker} HRV time domain - CNN \"  #\n",
    "g = sns.swarmplot(\n",
    "    data=data,\n",
    "    x=\"label_type\",\n",
    "    y=\"cv_mean_score\",\n",
    "    alpha=0.6,\n",
    "    dodge=True,\n",
    "    legend=False,\n",
    ")\n",
    "g.set_ylim((0.2, 1))\n",
    "g.set_title(title)\n",
    "\n",
    "df_means = (\n",
    "    data.groupby([\"label_type\", \"channel\"])[\"cv_mean_score\"].agg(\"mean\").reset_index()\n",
    ")\n",
    "\n",
    "pp = sns.pointplot(\n",
    "    x=\"label_type\",\n",
    "    y=\"cv_mean_score\",\n",
    "    data=df_means,\n",
    "    linestyles=\"\",\n",
    "    scale=2.5,\n",
    "    markers=\"_\",\n",
    "    order=[\"valence\", \"arousal\"],\n",
    ")\n",
    "sns.despine(bottom = True, left = True)\n",
    "g.axhline(0.5, color=\"red\", dashes=(2, 2))\n",
    "#sns.move_legend(pp, \"upper right\", bbox_to_anchor=(1.4, 1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a9260b4",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    load features from csv\n",
    "\"\"\"\n",
    "import shap\n",
    "\n",
    "from labels import (\n",
    "    get_tranformed_labels,\n",
    "    binary_label,\n",
    "    print_label_count,\n",
    "    get_categorical_labels,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "dir_name = \"eeg_features1\"\n",
    "result = load_data_from_csv(dir_name)\n",
    "\n",
    "all_label_array, label_list = get_labels_from_result(result)\n",
    "all_feature_array, feature_names = get_features_from_result(\n",
    "    result, [\"Subject\", \"Valence\", \"Arousal\", \"Attention\"], False\n",
    ")\n",
    "# all_feature_array = all_feature_array.drop([\"index\"], axis=1)\n",
    "\n",
    "# filter_pattern = \".*(?<!BETA2)$\"\n",
    "# only_specific_feature = \".*GAMMA$\"\n",
    "# all_feature_array = all_feature_array.filter(regex=only_specific_feature)\n",
    "feature_names = all_feature_array.columns\n",
    "print(all_feature_array.shape, len(feature_names), len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b61b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA  # Principal Component Analysis\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Scale each column in numer\n",
    "normalized_all_feature_array = pd.DataFrame(\n",
    "    scaler.fit_transform(all_feature_array), columns=all_feature_array.columns\n",
    ")\n",
    "\n",
    "\n",
    "# reduced_data = PCA(n_components=0.95).fit_transform(normalized_all_feature_array)\n",
    "# normalized_all_feature_array = pd.DataFrame(reduced_data)\n",
    "normalized_all_feature_array.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95037fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = get_tranformed_labels(all_label_array)\n",
    "label_list = get_categorical_labels(all_label_array, valence_threshold=0.6)\n",
    "valence_lables = binary_label(all_label_array[\"valence\"], 0.65)\n",
    "is_multi = False\n",
    "\n",
    "label_list = valence_lables\n",
    "print_label_count(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63db179",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_dmatrix = xgb.DMatrix(data=normalized_all_feature_array, label=np.array(label_list))\n",
    "\n",
    "# Creata the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematicallyvary the eta\n",
    "for curr_val in eta_vals:\n",
    "    params['eta'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n",
    "                        early_stopping_rounds=5, num_boost_round=10, metrics='rmse', seed=123, \n",
    "                       as_pandas=True)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results['test-rmse-mean'].tail().values[-1])\n",
    "    \n",
    "# Print the result DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=['eta', 'best_rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.5, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "eval_metric = [\"auc\",\"error\"]\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBClassifier(use_label_encoder=False, objective= 'binary:logistic',eval_metric=eval_metric)\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(param_grid=gbm_param_grid, estimator=gbm, \n",
    "                        scoring='accuracy', cv=5, verbose=2)\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(normalized_all_feature_array, np.array(label_list))\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "#print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_))) neg_mean_squared_error\n",
    "print(grid_mse.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6357fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"c\", \"y\", \"m\", \"r\"]\n",
    "\n",
    "\n",
    "accuracy = []\n",
    "gkf = GroupKFold()\n",
    "label_array = np.array(label_list)\n",
    "groups_list = [[i / NUM_LABEL_PER_SUBJECT] for i, j in enumerate(label_list)]\n",
    "group_array = np.hstack(groups_list)\n",
    "\n",
    "list_shap_values = list()\n",
    "list_test_sets = list()\n",
    "for train_index, val_index in gkf.split(\n",
    "    normalized_all_feature_array, label_array, groups=group_array\n",
    "):\n",
    "    train_features, train_labels = (\n",
    "        normalized_all_feature_array.iloc[train_index],\n",
    "        label_array[train_index],\n",
    "    )\n",
    "    val_features, val_labels = (\n",
    "        normalized_all_feature_array.iloc[val_index],\n",
    "        label_array[val_index],\n",
    "    )\n",
    "\n",
    "    # create model instance\n",
    "    model = XGBClassifier(n_estimators=2)\n",
    "    # fit model\n",
    "    model.fit(train_features, train_labels)\n",
    "    # Print accuracy.\n",
    "    acc = model.score(val_features, val_labels)\n",
    "    print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "    accuracy.append(acc)\n",
    "\n",
    "    # Summary plot\n",
    "    shap_values = shap.TreeExplainer(model).shap_values(val_features)\n",
    "    # for each iteration we save the test_set index and the shap_values\n",
    "    list_shap_values.append(shap_values)\n",
    "    list_test_sets.append(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining results from all iterations\n",
    "test_set = list_test_sets[0]\n",
    "shap_values = np.array(list_shap_values[0])\n",
    "for i in range(1, len(list_test_sets)):\n",
    "    test_set = np.concatenate((test_set, list_test_sets[i]), axis=0)\n",
    "    shap_values = (\n",
    "        np.concatenate((shap_values, np.array(list_shap_values[i])), axis=1)\n",
    "        if is_multi\n",
    "        else np.concatenate((shap_values, np.array(list_shap_values[i])), axis=0)\n",
    "    )  # for binary\n",
    "\n",
    "# bringing back variable names\n",
    "X_test = pd.DataFrame(\n",
    "    normalized_all_feature_array.iloc[test_set], columns=feature_names\n",
    ")\n",
    "\n",
    "# creating explanation plot for the whole experiment, the first dimension from shap_values indicate the class we are predicting (0=0, 1=1)\n",
    "if is_multi:\n",
    "    shap.summary_plot(shap_values[1], X_test)  # for multi i = class_num\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_test)  # for binary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
